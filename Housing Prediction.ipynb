{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2419e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display initial rows of the dataset\n",
    "print(\"\\nInitial Rows of the Dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f67c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029917e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Option 1: Drop rows with missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers (example: remove rows with outliers in numerical columns)\n",
    "df = df[(np.abs(stats.zscore(df.select_dtypes(include=['float64', 'int64']))) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cleaned dataset info\n",
    "print(\"\\nCleaned Dataset Information:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save cleaned data to a new CSV file in the current working directory\n",
    "df.to_csv('cleaned_traindata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ec7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data cleaning complete. Cleaned data saved to 'cleaned_traindata.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics for the dataset\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for the target variable (price)\n",
    "print(df['price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of house prices\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Distribution of house prices \n",
    "sns.histplot(df['price'], kde=True)\n",
    "plt.title('Distribution of Price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and kurtosis\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print('Skewness:', skew(df['price'].dropna()))\n",
    "print('Kurtosis:', kurtosis(df['price'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non-numeric values in numeric columns\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object:\n",
    "        print(f\"Non-numeric values in column {column}:\")\n",
    "        print(df[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert object columns to numeric using label encoding\n",
    "le = LabelEncoder()\n",
    "for column in df.select_dtypes(include=[object]):  # Object type usually indicates strings\n",
    "    df[column] = le.fit_transform(df[column].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types again\n",
    "print(df.dtypes)\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Verify the data is loaded correctly\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of 'bedrooms' vs 'price'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='bedrooms', y='price', data=data)\n",
    "plt.title('Bedrooms vs SalePrice')\n",
    "plt.xlabel('Bedrooms')\n",
    "plt.ylabel('SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e92016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of 'Price'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['price'], kde=True)\n",
    "plt.title('Distribution of price')\n",
    "plt.xlabel('price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of 'Area' vs 'Price'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='area', y='price', data=data)\n",
    "plt.title('area vs SalePrice')\n",
    "plt.xlabel('area')\n",
    "plt.ylabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for a few selected features\n",
    "selected_features = ['price', 'bathrooms', 'area', 'airconditioning']\n",
    "sns.pairplot(data[selected_features])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers in 'bathrooms'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=data['bathrooms'])\n",
    "plt.title('Box plot of bathrooms')\n",
    "plt.xlabel('bathrooms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers in 'Price'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=data['price'])\n",
    "plt.title('Box plot of price')\n",
    "plt.xlabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IQR for 'bathrooms'\n",
    "Q1 = data['bathrooms'].quantile(0.25)\n",
    "Q3 = data['bathrooms'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data based on IQR for 'bathrooms'\n",
    "data = data[(data['bathrooms'] >= lower_bound) & (data['bathrooms'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f215838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IQR for 'Price'\n",
    "Q1 = data['price'].quantile(0.25)\n",
    "Q3 = data['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data based on IQR for 'Price'\n",
    "data = data[(data['price'] >= lower_bound) & (data['price'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data size after removing outliers\n",
    "print(\"Size after removing outliers:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of 'bathrooms' after removing outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=data['bathrooms'])\n",
    "plt.title('Box plot of bathrooms (after removing outliers)')\n",
    "plt.xlabel('bathrooms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ba503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of 'Price' after removing outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=data['price'])\n",
    "plt.title('Box plot of price (after removing outliers)')\n",
    "plt.xlabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "# Load the dataset\n",
    "data = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Verify column names\n",
    "print(\"Column names in the dataset:\", data.columns)\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Create interaction features if 'bedrooms' and 'bathrooms' columns exist\n",
    "if 'bedrooms' in data.columns and 'bathrooms' in data.columns:\n",
    "    data['RoomsPerArea'] = data['bedrooms'] / data['bathrooms']\n",
    "else:\n",
    "    print(\"Columns 'bedrooms' and 'bathrooms' are missing. Check the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce326bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "data['RoomsPerArea'] = data['bedrooms'] / data['bathrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Polynomial Features\n",
    "data['bathrooms_Squared'] = data['bathrooms'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87407ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bins or categories from numerical features to capture non-linear relationships. For example, categorize house sizes into bins.\n",
    "data['areaBin'] = pd.cut(data['bathrooms'], bins=[0, 1000, 2000, 3000, 4000, 5000], labels=['Very Small', 'Small', 'Medium', 'Large', 'Very Large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f714740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Identify nominal and numerical features\n",
    "nominal_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "numerical_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "\n",
    "# Apply One-Hot Encoding for nominal features\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # Updated parameter name\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', onehot_encoder)])\n",
    "\n",
    "# Preprocessing for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "full_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nominal', nominal_transformer, nominal_features)])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Transform the features\n",
    "X_transformed = full_preprocessor.fit_transform(X)\n",
    "\n",
    "# Encode the target variable if it's categorical\n",
    "if y.dtype == 'object':  # Check if target is categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "else:\n",
    "    y_encoded = y\n",
    "\n",
    "# Print the shape of transformed data\n",
    "print(\"Transformed feature matrix shape:\", X_transformed.shape)\n",
    "print(\"Encoded target variable shape:\", y_encoded.shape)\n",
    "\n",
    "# Train a model (example)\n",
    "model = LinearRegression()\n",
    "model.fit(X_transformed, y_encoded)\n",
    "\n",
    "# Print a message to confirm the model has been trained\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization (Z-score Normalization)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Identify nominal and numerical features\n",
    "nominal_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "numerical_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "\n",
    "# Apply One-Hot Encoding for nominal features\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) \n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', onehot_encoder)])\n",
    "\n",
    "# Preprocessing for numerical features with standardization\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "full_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nominal', nominal_transformer, nominal_features)])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Transform the features\n",
    "X_transformed = full_preprocessor.fit_transform(X)\n",
    "\n",
    "# Encode the target variable if it's categorical\n",
    "if y.dtype == 'object':  # Check if target is categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "else:\n",
    "    y_encoded = y\n",
    "\n",
    "# Print the shape of transformed data\n",
    "print(\"Transformed feature matrix shape:\", X_transformed.shape)\n",
    "print(\"Encoded target variable shape:\", y_encoded.shape)\n",
    "\n",
    "# Train a model (example)\n",
    "model = LinearRegression()\n",
    "model.fit(X_transformed, y_encoded)\n",
    "\n",
    "# Print a message to confirm the model has been trained\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e23dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training feature matrix shape:\", X_train.shape)\n",
    "print(\"Testing feature matrix shape:\", X_test.shape)\n",
    "print(\"Training target variable shape:\", y_train.shape)\n",
    "print(\"Testing target variable shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb13632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose and Justify the Selection of Machine Learning Algorithms\n",
    "#1.Linear Regression:\n",
    "#Justification:It is simple and interpretable, suitable for a baseline model.\n",
    "#2.Decision Tree:\n",
    "#Justification:It can capture non-linear relationships, but may overfit if not tuned properly.\n",
    "#3.Gradient Boosting (e.g., XGBoost):\n",
    "#Justification: It often provides high performance and is effective for both linear and non-linear data, but can be more complex to tune.\n",
    "#4.Random Forest\n",
    "#Justification: It reduces overfitting by averaging multiple decision trees. Handles both linear and non-linear data well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf796b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Identify nominal and numerical features\n",
    "nominal_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "numerical_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "\n",
    "# Apply One-Hot Encoding for nominal features\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  \n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', onehot_encoder)\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical features with normalization\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())  # Use StandardScaler() for standardization\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "full_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nominal', nominal_transformer, nominal_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Transform the features\n",
    "X_transformed = full_preprocessor.fit_transform(X)\n",
    "\n",
    "# Encode the target variable if it's categorical\n",
    "if y.dtype == 'object':  # Check if target is categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "else:\n",
    "    y_encoded = y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  R²: {r2:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Identify nominal and numerical features\n",
    "nominal_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "numerical_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "\n",
    "# Apply One-Hot Encoding for nominal features\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # Updated parameter name\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', onehot_encoder)\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical features with normalization\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())  # Use StandardScaler() for standardization\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "full_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nominal', nominal_transformer, nominal_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Transform the features\n",
    "X_transformed = full_preprocessor.fit_transform(X)\n",
    "\n",
    "# Encode the target variable if it's categorical\n",
    "if y.dtype == 'object':  # Check if target is categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "else:\n",
    "    y_encoded = y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for Random Forest\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best score (neg MSE) for Random Forest:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Evaluate the best Random Forest model on the test set\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Evaluation\")\n",
    "print(f\"  RMSE: {rmse_rf:.2f}\")\n",
    "print(f\"  MAE: {mae_rf:.2f}\")\n",
    "print(f\"  R²: {r2_rf:.2f}\")\n",
    "\n",
    "# Define the parameter grid for Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for Gradient Boosting\n",
    "grid_search_gb = GridSearchCV(GradientBoostingRegressor(), param_grid_gb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Gradient Boosting:\", grid_search_gb.best_params_)\n",
    "print(\"Best score (neg MSE) for Gradient Boosting:\", grid_search_gb.best_score_)\n",
    "\n",
    "# Evaluate the best Gradient Boosting model on the test set\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "y_pred_gb = best_gb_model.predict(X_test)\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(\"Gradient Boosting Evaluation\")\n",
    "print(f\"  RMSE: {rmse_gb:.2f}\")\n",
    "print(f\"  MAE: {mae_gb:.2f}\")\n",
    "print(f\"  R²: {r2_gb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the Best-Performing Model\n",
    "#1.Compare Metrics: Compare the RMSE, MAE, and R² values of the Random Forest and Gradient Boosting models.\n",
    "#3.Choose the Best Model: Select the model with the lowest RMSE (or the highest R² if preferred).\n",
    "#4.Evaluate on Testing Set: Assess the performance of the selected model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Identify nominal and numerical features\n",
    "nominal_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "numerical_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "\n",
    "# Apply One-Hot Encoding for nominal features\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # Updated parameter name\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', onehot_encoder)\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical features with normalization\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())  # Use StandardScaler() for standardization\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "full_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nominal', nominal_transformer, nominal_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Transform the features\n",
    "X_transformed = full_preprocessor.fit_transform(X)\n",
    "\n",
    "# Encode the target variable if it's categorical\n",
    "if y.dtype == 'object':  # Check if target is categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "else:\n",
    "    y_encoded = y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for Random Forest\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for Gradient Boosting\n",
    "grid_search_gb = GridSearchCV(GradientBoostingRegressor(), param_grid_gb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best Random Forest model on the test set\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Evaluate the best Gradient Boosting model on the test set\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "y_pred_gb = best_gb_model.predict(X_test)\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "# Compare the performance metrics\n",
    "print(\"Random Forest Evaluation\")\n",
    "print(f\"  RMSE: {rmse_rf:.2f}\")\n",
    "print(f\"  MAE: {mae_rf:.2f}\")\n",
    "print(f\"  R²: {r2_rf:.2f}\\n\")\n",
    "\n",
    "print(\"Gradient Boosting Evaluation\")\n",
    "print(f\"  RMSE: {rmse_gb:.2f}\")\n",
    "print(f\"  MAE: {mae_gb:.2f}\")\n",
    "print(f\"  R²: {r2_gb:.2f}\\n\")\n",
    "\n",
    "# Select the best-performing model\n",
    "if rmse_gb < rmse_rf:\n",
    "    best_model = best_gb_model\n",
    "    best_model_name = 'Gradient Boosting'\n",
    "    best_rmse = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Interpretation and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7306684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best-performing model provides insights into which features most influence house prices. By understanding these critical features, stakeholders such as real estate agents, property developers, and buyers can make informed decisions. The model's performance metrics confirm its reliability and accuracy in predicting house prices, making it a valuable tool in the real estate market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importances of Interpreting Feature\n",
    "#Area: High importance might indicate that larger areas significantly increase house prices.\n",
    "#Bedrooms/Bathrooms: More bedrooms and bathrooms might correspond to higher house prices, indicating their value to buyers.\n",
    "#Parking: Availability of parking space can be a crucial factor in urban areas.\n",
    "#Airconditioning/Hotwaterheating: These features can enhance living conditions, thus increasing house prices.\n",
    "#Mainroad/Guestroom/Basement: Presence of these features might contribute to higher property values due to added convenience or space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Identify nominal and numerical features\n",
    "nominal_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "numerical_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "\n",
    "# Preprocessing pipeline for nominal features with One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', onehot_encoder)\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features with normalization\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())  # Use StandardScaler() for standardization if needed\n",
    "])\n",
    "\n",
    "# Combine numerical and nominal transformers into a single preprocessor\n",
    "full_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nominal', nominal_transformer, nominal_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Transform the features using the preprocessor\n",
    "X_transformed = full_preprocessor.fit_transform(X)\n",
    "\n",
    "# Encode the target variable if it's categorical\n",
    "if y.dtype == 'object':  # Check if target is categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "else:\n",
    "    y_encoded = y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the best model (Example with GradientBoostingRegressor)\n",
    "best_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5)  # Example parameters\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Model: Gradient Boosting\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  MAE: {mae:.2f}\")\n",
    "print(f\"  R²: {r2:.2f}\")\n",
    "\n",
    "# Extract feature importances from the best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Get the fitted OneHotEncoder from the full_preprocessor\n",
    "fitted_onehot_encoder = full_preprocessor.named_transformers_['nominal'].named_steps['onehot']\n",
    "\n",
    "# Get feature names from the fitted encoder\n",
    "onehot_feature_names = fitted_onehot_encoder.get_feature_names_out(nominal_features)\n",
    "feature_names = numerical_features + list(onehot_feature_names)\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importances in Gradient Boosting Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd93772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Identify nominal and numerical features\n",
    "nominal_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "numerical_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "\n",
    "# Preprocessing pipeline for nominal features with One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', onehot_encoder)\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features with normalization\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())  # Use StandardScaler() for standardization if needed\n",
    "])\n",
    "\n",
    "# Combine numerical and nominal transformers into a single preprocessor\n",
    "full_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('nominal', nominal_transformer, nominal_features)\n",
    "    ])\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Transform the features using the preprocessor\n",
    "X_transformed = full_preprocessor.fit_transform(X)\n",
    "\n",
    "# Encode the target variable if it's categorical\n",
    "if y.dtype == 'object':  # Check if target is categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "else:\n",
    "    y_encoded = y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the best model (Example with GradientBoostingRegressor)\n",
    "best_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5)  # Example parameters\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Model: Gradient Boosting\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  MAE: {mae:.2f}\")\n",
    "print(f\"  R²: {r2:.2f}\")\n",
    "\n",
    "# Extract feature importances from the best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Get the fitted OneHotEncoder from the full_preprocessor\n",
    "fitted_onehot_encoder = full_preprocessor.named_transformers_['nominal'].named_steps['onehot']\n",
    "\n",
    "# Get feature names from the fitted encoder\n",
    "onehot_feature_names = fitted_onehot_encoder.get_feature_names_out(nominal_features)\n",
    "feature_names = numerical_features + list(onehot_feature_names)\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importances in Gradient Boosting Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame with the preprocessed features for correlation matrix\n",
    "X_preprocessed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "\n",
    "# Add the target variable to the DataFrame\n",
    "X_preprocessed_df['price'] = y_encoded\n",
    "\n",
    "# Correlation Heatmap\n",
    "correlation_matrix = X_preprocessed_df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Actual vs. Predicted Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.show()\n",
    "\n",
    "# Residual Plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove the target variable from the feature lists if it exists\n",
    "if 'price' in numerical_features:\n",
    "    numerical_features.remove('price')\n",
    "if 'price' in categorical_features:\n",
    "    categorical_features.remove('price')\n",
    "\n",
    "# Define the preprocessing for numerical data (scaling)\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# Define the preprocessing for categorical data (one-hot encoding)\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Create a pipeline that first transforms the data then fits the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', gbr)\n",
    "])\n",
    "\n",
    "# Define the parameter grid for Gradient Boosting Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Model Performance on Test Set: RMSE={rmse}, MAE={mae}, R²={r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Evaluation\n",
    "# Import necessary libraries\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming best_model is the final model and X_test, y_test are the test datasets\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final Model Performance on Test Set:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Save the trained model and preprocessing pipeline\n",
    "joblib.dump(best_model, 'final_model.joblib')\n",
    "print(\"Model saved as 'final_model.joblib'\")\n",
    "\n",
    "# Save the preprocessing pipeline separately\n",
    "joblib.dump(best_model.named_steps['preprocessor'], 'preprocessor.joblib')\n",
    "print(\"Preprocessor saved as 'preprocessor.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a15e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'house_price_model.pkl'\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"File '{file_path}' found.\")\n",
    "else:\n",
    "    print(f\"File '{file_path}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab251d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Example model\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'house_price_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Example model\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "\n",
    "# Save the model\n",
    "model_path = 'house_price_model.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Verify the file creation\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"{model_path} has been successfully saved.\")\n",
    "else:\n",
    "    print(f\"{model_path} was not saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Example model\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "\n",
    "# Save the model\n",
    "model_path = r'C:\\Users\\USER\\Documents\\house_price_model.pkl'\n",
    "joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cdf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing\n",
    "numerical_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "categorical_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "\n",
    "# Define transformers\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model and preprocessor\n",
    "joblib.dump(model, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained model\n",
    "try:\n",
    "    model = joblib.dump(model, 'model.pkl')\n",
    "    print(f\"Model type: {type(model)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'model.joblib' not found. Please check the file path.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb10979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming `model` is your trained model\n",
    "joblib.dump(model, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load('model.joblib')\n",
    "print(f\"Loaded model type: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be3101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/USER/Downloads/Housing.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Example preprocessing\n",
    "# Identify categorical and numeric columns\n",
    "categorical_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "numeric_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
    "\n",
    "# Create transformers for preprocessing\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine preprocessing steps into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Prepare data\n",
    "X = data.drop('price', axis=1)  # Assuming 'price' is your target variable\n",
    "y = data['price']\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc690bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('model.joblib')\n",
    "\n",
    "def predict_from_input(area, bedrooms, bathrooms, stories, parking,\n",
    "                       mainroad, guestroom, basement, hotwaterheating,\n",
    "                       airconditioning, prefarea, furnishingstatus):\n",
    "    # Create DataFrame for prediction\n",
    "    df = pd.DataFrame([[\n",
    "        area, bedrooms, bathrooms, stories, parking,\n",
    "        mainroad, guestroom, basement, hotwaterheating,\n",
    "        airconditioning, prefarea, furnishingstatus\n",
    "    ]], columns=[\n",
    "        'area', 'bedrooms', 'bathrooms', 'stories', 'parking',\n",
    "        'mainroad', 'guestroom', 'basement', 'hotwaterheating',\n",
    "        'airconditioning', 'prefarea', 'furnishingstatus'\n",
    "    ])\n",
    "\n",
    "    # Make predictions\n",
    "    prediction = model.predict(df)\n",
    "    print(f'Predicted price: {prediction[0]}')\n",
    "\n",
    "# Example usage\n",
    "predict_from_input(1500, 3, 2, 2, 1, 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23462e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assume X_test and y_test are your test features and target values\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute performance metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c1715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(f\"Cross-Validation Scores: {-cv_scores}\")\n",
    "print(f\"Mean CV Score: {-cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Load and prepare the data\n",
    "data = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Define feature columns and target\n",
    "X = data[['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']]\n",
    "y = data['price']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']),\n",
    "        ('cat', OneHotEncoder(), ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus'])\n",
    "    ])\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbf3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the model\n",
    "pipeline = joblib.load('model.joblib')\n",
    "\n",
    "# Prepare input data\n",
    "input_data = pd.DataFrame({\n",
    "    'area': [1500],\n",
    "    'bedrooms': [3],\n",
    "    'bathrooms': [2],\n",
    "    'stories': [2],\n",
    "    'parking': [1],\n",
    "    'mainroad': ['yes'],  # Ensure categorical variables are in string format\n",
    "    'guestroom': ['no'],\n",
    "    'basement': ['no'],\n",
    "    'hotwaterheating': ['no'],\n",
    "    'airconditioning': ['yes'],\n",
    "    'prefarea': ['yes'],\n",
    "    'furnishingstatus': ['furnished']  # Example category name\n",
    "})\n",
    "\n",
    "# Convert categorical variables to the same format as used during training\n",
    "# Ensure categorical features are in the same encoding format used in the pipeline\n",
    "\n",
    "# Make predictions\n",
    "try:\n",
    "    predictions = pipeline.predict(input_data)\n",
    "    print(f'Predicted price: {predictions[0]}')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing a feedback loop:This involves several steps, including capturing new data, retraining the model, and updating the system with the improved model.\n",
    "#1.Capture New Data :You’ll need a way to capture and store new data. This could be through a user interface, an automated data collection system, or other means.\n",
    "import pandas as pd\n",
    "\n",
    "# Function to capture new data\n",
    "def capture_new_data(data_source):\n",
    "    new_data = pd.read_csv(data_source)  # Replace with your actual data source\n",
    "    return new_data\n",
    "\n",
    "# Example usage\n",
    "data_source = 'C:/Users/USER/AppData/Local/Temp/Rar$DI80.288/nigeria_houses_data.csv'\n",
    "new_data = capture_new_data(data_source)\n",
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the Training Dataset\n",
    "#Combine the new data with existing training data.\n",
    "# Load existing training data\n",
    "existing_data = pd.read_csv('C:/Users/USER/Downloads/Housing.csv')\n",
    "\n",
    "# Capture new data\n",
    "new_data = capture_new_data('C:/Users/USER/AppData/Local/Temp/Rar$DI80.288/nigeria_houses_data.csv')\n",
    "\n",
    "# Combine datasets\n",
    "updated_data = pd.concat([existing_data, new_data])\n",
    "\n",
    "# Optionally, save the updated dataset\n",
    "updated_data.to_csv('updated_training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain the Model\n",
    "#Retrain model using the updated dataset. \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Path to your CSV file\n",
    "file_path = 'C:/Users/USER/AppData/Local/Temp/Rar$DI80.288/nigeria_houses_data.csv'\n",
    "\n",
    "# Load the data\n",
    "new_data = pd.read_csv(file_path)\n",
    "\n",
    "# Identify and print non-numeric columns\n",
    "non_numeric_cols = new_data.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "label_encoders = {}\n",
    "for col in non_numeric_cols:\n",
    "    le = LabelEncoder()\n",
    "    new_data[col] = le.fit_transform(new_data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Assuming 'price' is the target variable\n",
    "X = new_data.drop('price', axis=1)\n",
    "y = new_data['price']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the retrained model\n",
    "joblib.dump(model, 'path_to_your_saved_model.pkl')\n",
    "\n",
    "print(\"Model retrained and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the System\n",
    "#Update system to use the new model for predictions.\n",
    "import os\n",
    "\n",
    "file_path = 'path_to_your_saved_model.pkl'\n",
    "if os.path.isfile(file_path):\n",
    "    print(f\"File found: {file_path}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9922a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'path_to_your_saved_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Path to your saved model\n",
    "model_path = 'path_to_your_saved_model.pkl'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(model_path):\n",
    "    print(\"File found:\", model_path)\n",
    "    try:\n",
    "        # Load the updated model\n",
    "        model = joblib.load(model_path)\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading model:\", e)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(new_data):\n",
    "    # Example preprocessing if needed\n",
    "    # new_data = preprocess(new_data)\n",
    "    \n",
    "    try:\n",
    "        # Make predictions\n",
    "        predictions = model.predict(new_data)\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        print(\"Error making predictions:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample new data\n",
    "sample_data = pd.DataFrame({\n",
    "    'bedrooms': [3],\n",
    "    'bathrooms': [2],\n",
    "    'toilets': [2],\n",
    "    'parking_space': [1],\n",
    "    'title': ['Sample Title'],\n",
    "    'town': ['Sample Town'],\n",
    "    'state': ['Sample State'],\n",
    "    })\n",
    "\n",
    "# Make predictions\n",
    "predictions = make_predictions(sample_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Example label encoders (these should match those used during training)\n",
    "label_encoders = {\n",
    "    'title': LabelEncoder(),\n",
    "    'town': LabelEncoder(),\n",
    "    'state': LabelEncoder()\n",
    "}\n",
    "\n",
    "# Function to preprocess new data\n",
    "def preprocess_new_data(new_data):\n",
    "    # Convert categorical columns to numerical using the label encoders\n",
    "    for column, encoder in label_encoders.items():\n",
    "        if column in new_data.columns:\n",
    "            new_data[column] = encoder.transform(new_data[column])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf87372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Load training data\n",
    "train_data = pd.read_csv('C:/Users/USER/updated_training_data.csv')\n",
    "\n",
    "# Define features and target\n",
    "features = ['title', 'town', 'state', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', \n",
    "            'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'furnishingstatus', 'toilets', \n",
    "            'parking_space']\n",
    "target = 'price'\n",
    "\n",
    "# Ensure all columns exist in the dataset\n",
    "existing_features = [col for col in features if col in train_data.columns]\n",
    "if not existing_features:\n",
    "    raise ValueError(\"None of the features are in the dataset columns\")\n",
    "\n",
    "X = train_data[existing_features].copy()\n",
    "y = train_data[target]\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute numerical features with mean\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "X[numerical_features] = num_imputer.fit_transform(X[numerical_features])\n",
    "\n",
    "# Impute categorical features with the most frequent value\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_features] = cat_imputer.fit_transform(X[categorical_features])\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for column in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X.loc[:, column] = le.fit_transform(X[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model and encoders\n",
    "joblib.dump(model, 'model.pkl')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "joblib.dump(num_imputer, 'num_imputer.pkl')\n",
    "joblib.dump(cat_imputer, 'cat_imputer.pkl')\n",
    "\n",
    "# Load the model and encoders\n",
    "model = joblib.load('model.pkl')\n",
    "label_encoders = joblib.load('label_encoders.pkl')\n",
    "num_imputer = joblib.load('num_imputer.pkl')\n",
    "cat_imputer = joblib.load('cat_imputer.pkl')\n",
    "\n",
    "def preprocess_new_data(new_data):\n",
    "    # Impute missing values\n",
    "    new_data[numerical_features] = num_imputer.transform(new_data[numerical_features])\n",
    "    new_data[categorical_features] = cat_imputer.transform(new_data[categorical_features])\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for column, encoder in label_encoders.items():\n",
    "        if column in new_data.columns:\n",
    "            # Handle unseen labels\n",
    "            new_data[column] = new_data[column].apply(lambda x: x if x in encoder.classes_ else encoder.classes_[0])\n",
    "            new_data[column] = encoder.transform(new_data[column])\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def make_predictions(new_data):\n",
    "    new_data = preprocess_new_data(new_data)\n",
    "    try:\n",
    "        predictions = model.predict(new_data)\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        print(f\"Error making predictions: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example new data for prediction\n",
    "new_data = pd.DataFrame({\n",
    "    'title': ['Sample Title'],\n",
    "    'town': ['Sample Town'],\n",
    "    'state': ['Sample State'],\n",
    "    'area': [1000],\n",
    "    'bedrooms': [3],\n",
    "    'bathrooms': [2],\n",
    "    'stories': [1],\n",
    "    'mainroad': [0],\n",
    "    'guestroom': [1],\n",
    "    'basement': [0],\n",
    "    'hotwaterheating': [1],\n",
    "    'airconditioning': [0],\n",
    "    'parking': [1],\n",
    "    'prefarea': [1],\n",
    "    'furnishingstatus': [0],\n",
    "    'toilets': [2],\n",
    "    'parking_space': [1]\n",
    "})\n",
    "\n",
    "# Make predictions\n",
    "predictions = make_predictions(new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5.3: Write a comprehensive report summarizing the project, including the methodology, results, and conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef791cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
